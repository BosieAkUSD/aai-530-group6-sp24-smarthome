{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preliminary EDA\n",
    "\n",
    "# Set the file path\n",
    "file_path = os.path.join(os.pardir, 'data_files', 'HomeA-meter4_2016.csv')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop 'gen [kW]' column\n",
    "df.drop(columns=['gen [kW]'], inplace=True)\n",
    "\n",
    "# Sum the values in each row and update 'use [kW]' column\n",
    "df['use [kW]'] = df.iloc[:, 2:].sum(axis=1)\n",
    "\n",
    "# Define the path for the cleaned data folder\n",
    "cleaned_data_folder = 'cleaned_data'\n",
    "\n",
    "# Check if the cleaned data folder exists, if not, create it\n",
    "if not os.path.exists(cleaned_data_folder):\n",
    "    os.makedirs(cleaned_data_folder)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file under the cleaned_data folder\n",
    "output_file_path = os.path.join(cleaned_data_folder, 'HomeA-meter4_2016_updated.csv')\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Create a file to store the output\n",
    "output_file = 'data_consistency_report.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data consistency report completed. Report saved to: data_consistency_report.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    # Display the first few rows of the dataset\n",
    "    f.write(\"Original Dataset:\\n\")\n",
    "    f.write(str(df.head()) + \"\\n\\n\")\n",
    "\n",
    "    # Data Cleaning Steps\n",
    "    # 1. Convert Date & Time column to datetime format\n",
    "    df['Date & Time'] = pd.to_datetime(df['Date & Time'])\n",
    "\n",
    "    # 2. Handle Missing Values (if any)\n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    f.write(\"Missing Values:\\n\")\n",
    "    f.write(str(missing_values) + \"\\n\\n\")\n",
    "\n",
    "    # 3. Check Data Consistency\n",
    "    # Check column data types\n",
    "    f.write(\"Data Types:\\n\")\n",
    "    f.write(str(df.dtypes) + \"\\n\\n\")\n",
    "\n",
    "    # Range checks for numerical columns\n",
    "    numerical_columns = df.select_dtypes(include=['float64']).columns\n",
    "    f.write(\"Range Checks:\\n\")\n",
    "    for col in numerical_columns:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "        f.write(f\"\\nRange for {col}:\\n\")\n",
    "        f.write(f\"Min: {min_val}, Max: {max_val}\\n\")\n",
    "\n",
    "    # Check unique values for categorical columns (if any)\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_columns) > 0:\n",
    "        f.write(\"\\nUnique Values for Categorical Columns:\\n\")\n",
    "        for col in categorical_columns:\n",
    "            unique_values = df[col].unique()\n",
    "            f.write(f\"\\n{col}:\\n\")\n",
    "            f.write(str(unique_values) + \"\\n\")\n",
    "\n",
    "    # Temporal consistency: Check for duplicate timestamps\n",
    "    duplicate_timestamps = df['Date & Time'].duplicated().sum()\n",
    "    f.write(\"\\nDuplicate Timestamps: \" + str(duplicate_timestamps) + \"\\n\\n\")\n",
    "\n",
    "print(\"Data consistency report completed. Report saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data normalization completed.\n"
     ]
    }
   ],
   "source": [
    "# 4. Normalize Data\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Select numerical columns to normalize\n",
    "numerical_columns = df.select_dtypes(include=['float64']).columns\n",
    "\n",
    "# Apply normalization to numerical columns\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "print(\"Data normalization completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: use [kW], Lower Bound: -0.04228294301156803, Upper Bound: 0.13208863069662286\n",
      "Column: KitchenDenLights [kW], Lower Bound: -0.3128392327915092, Upper Bound: 0.5678947104704457\n",
      "Column: MasterBedBathLights [kW], Lower Bound: -0.0944027800629083, Upper Bound: 0.20594402855068955\n",
      "Column: MasterOutlets [kW], Lower Bound: 0.0048460198730595535, Upper Bound: 0.006860590609229876\n",
      "Column: DenOutdoorLights [kW], Lower Bound: 0.021921660038775007, Upper Bound: 0.03311568968067742\n",
      "Column: DenOutlets [kW], Lower Bound: -0.00817198471581757, Upper Bound: 0.013792243486629092\n",
      "Column: RearBasementLights [kW], Lower Bound: 0.034369575129533675, Upper Bound: 0.04127809326424872\n",
      "Column: KitchenOutletsEast [kW], Lower Bound: 0.0019562340848806358, Upper Bound: 0.0023983189655172417\n",
      "Column: KitchenOutletsSouth [kW], Lower Bound: 0.001181095369471238, Upper Bound: 0.001394225360954769\n",
      "Column: DishwasherDisposalSinkLight [kW], Lower Bound: -0.024102477724538038, Upper Bound: 0.04443823664831721\n",
      "Column: Refrigerator [kW], Lower Bound: -0.12487740679537304, Upper Bound: 0.22238791906815528\n",
      "Column: Microwave [kW], Lower Bound: 0.002056454671756248, Upper Bound: 0.002506710797905743\n",
      "Column: OfficeLights [kW], Lower Bound: 0.008390771897295337, Upper Bound: 0.07497950985529445\n"
     ]
    }
   ],
   "source": [
    "# 5. Outlier Detection\n",
    "\n",
    "# Define a function to detect outliers using the interquartile range (IQR) method\n",
    "def detect_outliers(df, threshold=1.5):\n",
    "    outliers = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['float64', 'int64']:\n",
    "            Q1 = np.percentile(df[col], 25)\n",
    "            Q3 = np.percentile(df[col], 75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "            # Print the calculated bounds for debugging\n",
    "            print(f\"Column: {col}, Lower Bound: {lower_bound}, Upper Bound: {upper_bound}\")\n",
    "            # Identify outliers and append their indices\n",
    "            outliers.extend(df[(df[col] < lower_bound) | (df[col] > upper_bound)].index)\n",
    "    return list(set(outliers))\n",
    "\n",
    "# Detect outliers in the dataset\n",
    "outlier_indices = detect_outliers(df)\n",
    "\n",
    "# Remove outliers from the dataset\n",
    "cleaned_df = df.drop(outlier_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Feature Engineering (if necessary)\n",
    "# Extract additional temporal features: day of the week and hour of the day\n",
    "cleaned_df['DayOfWeek'] = cleaned_df['Date & Time'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "cleaned_df['HourOfDay'] = cleaned_df['Date & Time'].dt.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (321755, 13) (321755,)\n",
      "Validation set: (80439, 13) (80439,)\n",
      "Test set: (100549, 13) (100549,)\n"
     ]
    }
   ],
   "source": [
    "# 7. Data Splitting\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df.drop(columns=['Date & Time'])  # Features (exclude 'Date & Time' column)\n",
    "y = df['use [kW]']  # Target variable\n",
    "\n",
    "# Split the dataset into training (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training set into training (80%) and validation (20%) sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning process completed. Cleaned dataset saved to: cleaned_data/HomeA-meter4_2016_updated.csv\n"
     ]
    }
   ],
   "source": [
    "# Set the cleaned file path\n",
    "cleaned_file_path = os.path.join('cleaned_data', 'HomeA-meter4_2016_updated.csv')\n",
    "\n",
    "# Save the cleaned dataset under the cleaned_data directory\n",
    "df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(\"\\nCleaning process completed. Cleaned dataset saved to:\", cleaned_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
