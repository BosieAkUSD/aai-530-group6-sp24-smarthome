{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Imports**:\n",
    "   - We import necessary libraries and modules:\n",
    "     - `pandas`: For data manipulation and analysis.\n",
    "     - `numpy`: For numerical computations.\n",
    "     - `MinMaxScaler` from `sklearn.preprocessing`: For feature scaling.\n",
    "     - `train_test_split` from `sklearn.model_selection`: For splitting the dataset into training and testing sets.\n",
    "     - `tensorflow` and its submodules: For building and training neural networks.\n",
    "     - `matplotlib.pyplot`: For data visualization.\n",
    "\n",
    "2. **Load Data**:\n",
    "   - We load the cleaned dataset from the CSV file using `pd.read_csv()`.\n",
    "\n",
    "3. **Data Preprocessing**:\n",
    "   - We scale the input features using `MinMaxScaler` to ensure they are on a similar scale.\n",
    "   - The dataset is split into training, validation, and test sets using `train_test_split()`.\n",
    "\n",
    "4. **Model Architecture - DNN**:\n",
    "   - We define the architecture of the Deep Neural Network (DNN) model using `Sequential()` from TensorFlow.keras.\n",
    "   - The model consists of multiple dense (fully connected) layers with ReLU activation functions.\n",
    "   - BatchNormalization layers are added to normalize the activations of the previous layer.\n",
    "   - Dropout layers are added to mitigate overfitting by randomly dropping a fraction of the connections during training.\n",
    "\n",
    "5. **Compile the Model**:\n",
    "   - We compile the model using the Adam optimizer and Mean Squared Error (MSE) loss function.\n",
    "\n",
    "6. **Callbacks**:\n",
    "   - EarlyStopping: Monitor the validation loss and stop training if there is no improvement for a certain number of epochs to prevent overfitting.\n",
    "   - ReduceLROnPlateau: Reduce the learning rate if the validation loss plateaus to help the model converge better.\n",
    "\n",
    "7. **Model Training**:\n",
    "   - We train the DNN model using the training data and validate it using the validation data.\n",
    "   - The model is trained for a maximum of 50 epochs with a batch size of 64 samples.\n",
    "\n",
    "8. **Evaluation**:\n",
    "   - We evaluate the trained model's performance on the test set using the test data.\n",
    "   - The test loss (MSE) is computed and printed to assess the model's generalization performance.\n",
    "\n",
    "9. **Visualization**:\n",
    "   - We plot the training and validation loss curves to visualize the training progress and identify potential overfitting.\n",
    "\n",
    "10. **Prediction**:\n",
    "    - Finally, we make predictions on the test set using the trained DNN model.\n",
    "\n",
    "This code implements a Deep Neural Network model for energy consumption prediction in smart home environments, incorporating advanced techniques such as batch normalization, dropout regularization, and early stopping. It aims to achieve accurate and robust predictions while preventing overfitting and ensuring efficient training convergence. Adjustments to hyperparameters and architecture can be made based on experimentation and evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 21:20:04.655637: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6285/6285 [==============================] - 17s 2ms/step - loss: 0.0392 - val_loss: 8.7401e-05 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "6285/6285 [==============================] - 15s 2ms/step - loss: 4.9497e-04 - val_loss: 1.2830e-04 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "6285/6285 [==============================] - 16s 3ms/step - loss: 3.0375e-04 - val_loss: 7.3324e-05 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "6285/6285 [==============================] - 15s 2ms/step - loss: 2.1936e-04 - val_loss: 8.0542e-05 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "6285/6285 [==============================] - 14s 2ms/step - loss: 1.7262e-04 - val_loss: 7.8706e-05 - lr: 2.0000e-04\n",
      "Epoch 6/50\n",
      "6285/6285 [==============================] - 15s 2ms/step - loss: 1.6960e-04 - val_loss: 1.0968e-04 - lr: 2.0000e-04\n",
      "Epoch 7/50\n",
      "6285/6285 [==============================] - 16s 3ms/step - loss: 1.6324e-04 - val_loss: 1.1667e-04 - lr: 2.0000e-04\n",
      "Epoch 8/50\n",
      "6285/6285 [==============================] - 16s 3ms/step - loss: 1.5775e-04 - val_loss: 9.0992e-05 - lr: 4.0000e-05\n",
      " 570/1572 [=========>....................] - ETA: 1s - loss: 7.2847e-05"
     ]
    }
   ],
   "source": [
    "# Load the cleaned data\n",
    "cleaned_data_file = '../data_cleaning_module/cleaned_data/HomeA-meter4_2016_updated.csv'\n",
    "df = pd.read_csv(cleaned_data_file)\n",
    "\n",
    "# Data Preprocessing\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(df.drop(columns=['Date & Time']))\n",
    "y = df['use [kW]']\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Model Architecture - DNN\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Define callbacks for early stopping and learning rate reduction\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "\n",
    "# Evaluation\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "\n",
    "# Plot training and validation loss curves\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Prediction\n",
    "y_pred = model.predict(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
