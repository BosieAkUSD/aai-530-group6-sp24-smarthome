import pandas as pd
import os
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
# Get the absolute path of the directory containing the notebook file
notebook_dir = os.path.dirname(os.path.abspath('__file__'))

# Navigate two levels up to reach the 'data_files' directory
data_files_dir = os.path.abspath(os.path.join(notebook_dir, os.pardir, os.pardir, 'data_files'))

# Construct the path to the file
file_path = os.path.join(data_files_dir, 'HomeA-meter4_2016.csv')

# Load the dataset
df = pd.read_csv(file_path)

# Drop 'gen [kW]' and 'use [kW]' columns
df.drop(columns=['gen [kW]', 'use [kW]'], inplace=True)

# Define the path for the cleaned data folder
cleaned_data_folder = 'output_cleaned_dnn_data'

# Check if the cleaned data folder exists, if not, create it
if not os.path.exists(cleaned_data_folder):
    os.makedirs(cleaned_data_folder)

# Save the updated DataFrame to a new TXT file under the cleaned_data folder
dataset_output_file_path = os.path.join(cleaned_data_folder, 'DNN_HomeA-meter4_2016_updated.txt')

# Save the cleaned dataset under the cleaned_data directory
df.to_csv(dataset_output_file_path, sep='\t', index=True)


# Create a file to store the output
output_file = os.path.join(cleaned_data_folder, 'DNN_data_consistency_report.txt')

with open(output_file, 'w') as f:
    f.write("Original Dataset:\n")
    f.write(str(df.head()) + "\n\n")

    # Data Cleaning Steps
    # 1. Convert Date & Time column to datetime format
    df['Date & Time'] = pd.to_datetime(df['Date & Time'])

    # 2. Handle Missing Values (if any)
    # Check for missing values
    missing_values = df.isnull().sum()
    f.write("Missing Values:\n")
    f.write(str(missing_values) + "\n\n")

    # 3. Check Data Consistency
    # Check column data types
    f.write("Data Types:\n")
    f.write(str(df.dtypes) + "\n\n")

    # Range checks for numerical columns
    numerical_columns = df.select_dtypes(include=['float64']).columns
    f.write("Range Checks:\n")
    for col in numerical_columns:
        min_val = df[col].min()
        max_val = df[col].max()
        f.write(f"\nRange for {col}:\n")
        f.write(f"Min: {min_val}, Max: {max_val}\n")

    # Check unique values for categorical columns (if any)
    categorical_columns = df.select_dtypes(include=['object']).columns
    if len(categorical_columns) > 0:
        f.write("\nUnique Values for Categorical Columns:\n")
        for col in categorical_columns:
            unique_values = df[col].unique()
            f.write(f"\n{col}:\n")
            f.write(str(unique_values) + "\n")

    # Temporal consistency: Check for duplicate timestamps
    duplicate_timestamps = df['Date & Time'].duplicated().sum()
    f.write("\nDuplicate Timestamps: " + str(duplicate_timestamps) + "\n\n")

print("Data consistency report completed. Report saved to:", output_file)

# 4. Normalize Data

# Create a MinMaxScaler object
scaler = MinMaxScaler()

# Select numerical columns to normalize
numerical_columns = df.select_dtypes(include=['float64']).columns

# Apply normalization to numerical columns
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

print("Data normalization completed.")
# 5. Outlier Detection

# Define a function to detect outliers using the interquartile range (IQR) method
def detect_outliers(df, threshold=1.5):
    outliers = []
    for col in df.columns:
        if df[col].dtype in ['float64', 'int64']:
            Q1 = np.percentile(df[col], 25)
            Q3 = np.percentile(df[col], 75)
            IQR = Q3 - Q1
            lower_bound = Q1 - threshold * IQR
            upper_bound = Q3 + threshold * IQR
            # Print the calculated bounds for debugging
            print(f"Column: {col}, Lower Bound: {lower_bound}, Upper Bound: {upper_bound}")
            # Identify outliers and append their indices
            outliers.extend(df[(df[col] < lower_bound) | (df[col] > upper_bound)].index)
    return list(set(outliers))

# Detect outliers in the dataset
outlier_indices = detect_outliers(df)

# Remove outliers from the dataset
cleaned_df = df.drop(outlier_indices)

# 6. Feature Engineering (if necessary)
# Extract additional temporal features: day of the week and hour of the day
cleaned_df['DayOfWeek'] = cleaned_df['Date & Time'].dt.dayofweek  # Monday=0, Sunday=6
cleaned_df['HourOfDay'] = cleaned_df['Date & Time'].dt.hour

# Sum the values in each row starting from 'KitchenDenLights [kW]' column and update 'Total Energy Demand' column
df['Total Energy Demand'] = df.iloc[:, 1:].sum(axis=1)

# Assuming 'Date & Time' is the name of your datetime column
df['Date & Time'] = pd.to_datetime(df['Date & Time'])

# Set the datetime column as the index
df.set_index('Date & Time', inplace=True)

# Now, you can proceed with resampling and plotting
df_weekly = df['Total Energy Demand'].resample('W').sum()


# Data Exploration
df.info()
print(df.head())

# Plot the total energy demand
df_weekly.plot(figsize=(10, 6))
plt.ylabel('Energy Demand (kW)')
plt.title('Total Energy Demand Over Time')
plt.show()

# Set the cleaned file path
cleaned_file_path = os.path.join(cleaned_data_folder, 'DNN_HomeA-meter4_2016_updated.txt')
cleaned_file_path_csv = os.path.join(cleaned_data_folder, 'DNN_HomeA-meter4_2016_updated.csv')

# Save the cleaned dataset under the cleaned_data directory
df.to_csv(cleaned_file_path, sep='\t', index=True)

df.to_csv(cleaned_file_path_csv, index=True)


df.head()
